---
layout: page
title: Data Preparation and Engineering 
importance: 1
category: work
giscus_comments: false
---

# Data Preparation and Engineering

In order to assess the compliance rate over the years, a specialized pipeline was developed to convert unstructured SEC EDGAR filings into a clean, longitudinal dataset. This process involved several critical steps to ensure data integrity over the 22-year study period.

**Identity Verification via Accession Numbers**

The primary challenge in processing legacy filings is "field displacement," where names and ID numbers are often misaligned in the original text. To resolve this, the system used the SEC Accession Number as a "truth anchor". Because the first ten digits of this unique ID are automatically assigned to the filer, the pipeline could verify the identity of the Reporting Owner even when the text within the document was disorganized.

**Heuristic Extraction and Master Verification**

The pipeline scanned the header of each document to identify transaction dates and stock tickers. To prevent the inclusion of "noise" or false data, every extracted Central Index Key (CIK) was cross-checked against the SEC Master Directory. If a number found in the text did not match an official entity in the master list, it was excluded from the study to maintain high precision .

**Categorization and Storage Efficiency**

The software distinguished between individual and institutional entities by searching for corporate markers such as "LLC" or "Corp" in the names. This allowed for a detailed comparison of reporting behavior between different types of market participants. Finally, the data was migrated to a partitioned Parquet format, which reduced the storage size by 70% and enabled efficient computation for the 110,000 entities in the final sample.

**Limitations of Human-Driven Compliance**

A significant technical hurdle is the persistent "Compliance Ceiling" observed since the introduction of the Sarbanes-Oxley 2-day mandate in 2002 . Because the reconciliation pipeline shows that reporting delays have not improved over two decades, it suggests a structural failure in manual reporting methods.


[Read For Data Description]({{ '/projects/20260206/' | relative_url }})





